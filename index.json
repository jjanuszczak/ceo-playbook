[{"content":"Test 123\n","date":"22 November 2025","externalUrl":null,"permalink":"/ceo-playbook/about/","section":"The CEO Playbook","summary":"","title":"About","type":"page"},{"content":"","date":"22 November 2025","externalUrl":null,"permalink":"/ceo-playbook/articles/","section":"Articles","summary":"","title":"Articles","type":"articles"},{"content":"","date":"22 November 2025","externalUrl":null,"permalink":"/ceo-playbook/tags/design/","section":"Tags","summary":"","title":"Design","type":"tags"},{"content":"Git is a case study on cleverly, if not easthetically, designing around constraints vs throwing hardware at a problem\nGit stores objects (blobs, trees, commits, and tags) in the .git/objects directory using a loose object format by default. Each object is identified by its SHA-1 hash (a 40-character hexadecimal string, e.g., a1b2c3d4... for SHA-1; Git now uses SHA-256 in some cases, but the structure is similar).\nWhen storing a loose object:\nThe first two characters of the hash become a subdirectory name (e.g., a1). The remaining characters become the filename inside that subdirectory (e.g., b2c3d4...). So an object with hash a1b2c3d4e5f6... ends up at .git/objects/a1/b2c3d4e5f6....\nWhy This Specific Structure? # This is a deliberate performance optimization for filesystem efficiency:\nAvoids huge flat directories\nMany filesystems (especially ext3/ext4 in older configurations, ReiserFS, NTFS, and others common in the mid-2000s when Git was created) slow down dramatically or even have hard limits when a single directory contains tens or hundreds of thousands of files.\nDirectory listing (readdir), lookups, and metadata operations become O(n) or worse. Real-world examples: the Linux kernel repo can have millions of objects over time; putting them all directly under .git/objects/ would make operations painfully slow. \u0026ldquo;Fan-out\u0026rdquo; with exactly 256 subdirectories\nUsing the first two hex characters creates 256 possible subdirectories (00 to ff).\nThis spreads objects roughly evenly (assuming uniform hash distribution). Even in a repository with millions of objects, each subdirectory typically holds only a few thousand files — a number most filesystems handle efficiently. The top-level .git/objects/ directory itself stays tiny (just 256 entries + pack/ and info/). One level is enough; deeper would be overkill\nTwo characters (1 byte) → 256 buckets → sufficient for virtually all repositories. Using three characters (4096 subdirectories) would add unnecessary depth and overhead for typical repos. Linus Torvalds and early Git developers chose this as a pragmatic sweet spot based on real filesystem behavior at the time. Bonus: Fast partial-hash lookups\nWhen you type a short unique hash (e.g., git show a1b2c3), Git only has to look in one specific subdirectory (.git/objects/a1/) and do a simple filename prefix match — very fast, even without an index.\nWhat Happens Later? # New objects start as loose (one file per object). Over time, git gc packs them into efficient packfiles (.git/objects/pack/) for storage and transfer, where this directory structure is no longer used. The fan-out is mainly for the loose phase.\nThis design has proven extremely effective and is why Git repositories remain fast even when they contain hundreds of thousands or millions of objects.\nThe Big Picture # The Git object layout is a classic case of \u0026ldquo;design around the actual constraints\u0026rdquo; instead of assuming \u0026ldquo;hardware will save us.\u0026rdquo;\nIn 2005, when Linus built Git:\nSpinning hard drives were slow at random seeks and directory lookups. Common filesystems (ext3, NTFS, etc.) choked badly once a directory crossed ~10,000–20,000 entries. A busy Linux kernel repository could easily generate hundreds of thousands of objects over time. SSDs basically didn’t exist for consumers, and even if you had fast hardware, many developers were on laptops or shared servers with mediocre disks. Throwing \u0026ldquo;faster hardware\u0026rdquo; at it wouldn’t have helped most users, and it certainly wouldn’t have made Git usable on large projects back then. Instead, Linus chose a tiny, zero-cost software fix: split into 256 subdirectories using the first two hex digits of the hash. It costs almost nothing in code complexity, adds no runtime overhead, and completely sidesteps the filesystem pathology.\nIt’s a textbook example of thoughtful systems design:\nSolves the real bottleneck (filesystem behavior) rather than a symptom. Scales from tiny repos to monstrous ones with no configuration changes. Still beneficial today even on NVMe SSDs and modern filesystems (fewer directory entries = fewer inodes, less metadata pressure, faster clones on network filesystems, etc.). You see the same philosophy elsewhere in Git: content-addressed storage, Merkle-tree history, packfiles, delta compression… almost everything is built to be stingy with I/O and robust on slow or unreliable hardware, because that was the reality when it was born. Faster CPUs and SSDs are nice bonuses, but Git didn’t need to wait for them to be blazingly fast.\n","date":"22 November 2025","externalUrl":null,"permalink":"/ceo-playbook/articles/git-design/","section":"Articles","summary":"","title":"Designing Around Constraints","type":"articles"},{"content":"","date":"22 November 2025","externalUrl":null,"permalink":"/ceo-playbook/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"22 November 2025","externalUrl":null,"permalink":"/ceo-playbook/tags/technology/","section":"Tags","summary":"","title":"Technology","type":"tags"},{"content":"","date":"22 November 2025","externalUrl":null,"permalink":"/ceo-playbook/","section":"The CEO Playbook","summary":"","title":"The CEO Playbook","type":"page"},{"content":"","date":"24 September 2025","externalUrl":null,"permalink":"/ceo-playbook/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"24 September 2025","externalUrl":null,"permalink":"/ceo-playbook/tags/banking/","section":"Tags","summary":"","title":"Banking","type":"tags"},{"content":"","date":"24 September 2025","externalUrl":null,"permalink":"/ceo-playbook/tags/digital/","section":"Tags","summary":"","title":"Digital","type":"tags"},{"content":"","date":"31 October 2022","externalUrl":null,"permalink":"/ceo-playbook/tags/defi/","section":"Tags","summary":"","title":"DeFi","type":"tags"},{"content":"","date":"31 October 2022","externalUrl":null,"permalink":"/ceo-playbook/tags/embedded-finance/","section":"Tags","summary":"","title":"Embedded Finance","type":"tags"},{"content":"","date":"31 October 2022","externalUrl":null,"permalink":"/ceo-playbook/tags/open-finance/","section":"Tags","summary":"","title":"Open Finance","type":"tags"},{"content":"","date":"13 May 2018","externalUrl":null,"permalink":"/ceo-playbook/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","externalUrl":null,"permalink":"/ceo-playbook/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ceo-playbook/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/ceo-playbook/series/","section":"Series","summary":"","title":"Series","type":"series"}]